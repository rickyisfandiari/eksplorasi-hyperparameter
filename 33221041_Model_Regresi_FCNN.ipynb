{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aYMHjd5GAE4"
      },
      "source": [
        "# **Model Regresi Harga Perumahan Boston Menggunakan FCNN**\n",
        "\n",
        "Penelitian ini akan memprediksi harga rata-rata rumah di pinggiran kota Boston tertentu pada pertengahan 1970-an menggunakan metode fully connected neural network (FCNN). Beberapa faktor yang mempengaruhi harga rata-rata rrumah di pinggiran kota pada saat itu adalah tingkat kejahatan, tarif pajak properti lokal, dan lain sebagainya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0T5uUuZGAFL"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Dataset yang akan digunakan memiliki total series 506, dan terbagi menjadi 404 sampel pelatihan dan 102 sampel pengujian. Setiap fitur dalam data input (seperti tingkat kejahatan) memiliki skala yang berbeda-beda. Misalnya proporsi, yang mengambil nilai antara 0 dan 1, ada juga yang mengambil nilai antara 1 dan 12, dan ada juga yang mengambil nilai antara 0 dan 100. Dataset yang akan digunakan memiliki 404 sampel pelatihan dan 102 sampel pengujian. Dataset ini memiliki 13 fitur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I04Tzn1oGAFS"
      },
      "outputs": [],
      "source": [
        "# Import library Keras terlebih dahulu.\n",
        "import keras\n",
        "from keras.datasets import boston_housing\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqslvhD89LZr"
      },
      "outputs": [],
      "source": [
        "# Load dataset yang akan digunakan.\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqTSYqE8GAF9"
      },
      "outputs": [],
      "source": [
        "# Jumlah data training sebanyak 404 sampel dan memiliki 13 fitur.\n",
        "train_data[1],train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-ErtmN4GAGq"
      },
      "outputs": [],
      "source": [
        "# Jumlah data testing sebanyak 102 sampel dan memiliki 13 fitur.\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8-V62_bGAHV"
      },
      "outputs": [],
      "source": [
        "# Targetnya adalah nilai median rumah yang ditempati pemilik (dalam ribuan dolar).\n",
        "# Harga berkisar antara $ 10.000 hingga $ 50.000.\n",
        "import pandas as pd\n",
        "df_train_targets = pd.DataFrame(train_targets)\n",
        "train_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlkzTesbCdNM"
      },
      "outputs": [],
      "source": [
        "# Awalnya peneliti fokus pada variabel terikat, karena sifat variabel terikat sangat penting\n",
        "# untuk pemilihan model, yaitu variabel harga rumah yang juga jenis variabel kontinyu.\n",
        "# Untuk memahaminya kita plot terlebih dahulu distribusinya.\n",
        "# Data train_targets diplot ke histogram terlebih dahulu menggunakan library Seaborn.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.distplot(df_train_targets, color='red');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB7N6BYQcsks"
      },
      "outputs": [],
      "source": [
        "# Kemudian dilihat sebaran variabel bebasnya (fitur) sebanyak 13 variabel.\n",
        "# Histogram masing-masing fitur dapat dilihat pada grafik di bagian diagonal.\n",
        "# Data train_data diplot menggunakan library Seaborn.\n",
        "import pandas as pd\n",
        "df_train_data = pd.DataFrame(train_data)\n",
        "sns.pairplot(df_train_data);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8gWyc3eGAIP"
      },
      "outputs": [],
      "source": [
        "# Kemudian dilakukan normalisasi fitur. Untuk setiap fitur dalam data input,\n",
        "# peneliti mengurangi rata-rata fitur dan membaginya dengan standar deviasi,\n",
        "# sehingga fitur akan berpusat di sekitar 0 dan memiliki simpangan baku yang standar.\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyFTJ6RVGAIy"
      },
      "source": [
        "## Pemodelan\n",
        "Karena sampel tergolong kecil, peneliti menggunakan network yang kecil dengan 2 hidden layer, masing-masing sebanyak 128 dense unit (kondisi optimal). Umumnya, semakin sedikit data pelatihan maka akan semakin overfitting. Untuk itu, menggunakan network kecil merupakan salah satu cara untuk mengurangi overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgEuHl2wGAI8"
      },
      "outputs": [],
      "source": [
        "# Peneliti menggunakan fungsi dari library Keras untuk membangun model.\n",
        "# Optimizer yang digunakan adalah RMSprop dari library Keras.\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu',\n",
        "                           input_shape=(train_data.shape[1],)))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi struktur model.\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='struktur_model_fcnn.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "Os-ztGPDLO-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ringkasan struktur model.\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "RoqfXkWuMniZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUAV0MqtGAJW"
      },
      "source": [
        "## Pelatihan dan Validasi\n",
        "\n",
        "Untuk mengevaluasi model jaringannya, kami terus menyesuaikan parameternya (seperti jumlah epoch yang digunakan untuk pelatihan). Data dibagi menjadi satu set pelatihan dan satu set validasi. Namun, karena datanya tergolong sangat sedikit, set validasi akan menjadi sangat kecil (sekitar 100 sampel). Konsekuensinya adalah skor validasi dapat berubah banyak tergantung pada titik data mana yang digunakan untuk validasi dan yang digunakan untuk pelatihan. Skor validasi kemungkinan juga akan memiliki varians tinggi sehubungan dengan hal tersebut.\n",
        "\n",
        "Dari beberapa sumber, praktik terbaik untuk situasi seperti itu adalah menggunakan validasi silang K-Fold, yaitu membagi data yang tersedia menjadi K partisi (umumnya K=4 atau K=5), kemudian membuat model K yang identik, dan melatih masing-masing pada partisi K-1 sambil mengevaluasi partisi yang tersisa. Nilai validasi untuk model yang digunakan selanjutnya akan menjadi rata-rata dari nilai validasi K yang diperoleh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFts6rAHGAM2"
      },
      "outputs": [],
      "source": [
        "# Peneliti melatih network menggunakan epoch 100 dengan fold 5 (kondisi optimal).\n",
        "# Untuk mencatat seberapa baik model di setiap epoch, log skor validasi akan\n",
        "# disimpan untuk setiap epoch.\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "# Memory clean-up.\n",
        "K.clear_session()\n",
        "\n",
        "k = 5\n",
        "num_val_samples = len(train_data) // k\n",
        "\n",
        "num_epoch = 100\n",
        "num_batch_size = 5\n",
        "all_mae_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "    print('Memproses Fold #', i+1)\n",
        "    # Menyiapkan data validasi (data dari partisi ke-K).\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    # Menyiapkan data pelatihan (data dari semua partisi lain).\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    # Membangun model dengan fungsi dari library Keras.\n",
        "    model = build_model()\n",
        "    # Melatih model (dalam silent mode, verbose=0).\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epoch, batch_size=num_batch_size, verbose=0)\n",
        "    mae_history = history.history['val_mae']\n",
        "    all_mae_histories.append(mae_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lBAkLa-GANz"
      },
      "outputs": [],
      "source": [
        "# Menghitung rata-rata skor MAE per epoch untuk semua fold.\n",
        "average_mae_history = [\n",
        "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epoch)]\n",
        "    \n",
        "#average_mae_history\n",
        "np.mean(average_mae_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QXIsVS3GAOQ"
      },
      "outputs": [],
      "source": [
        "# Kemudian hasilnya diplot.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE Hasil Validasi')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocXAez3MZ9mZ"
      },
      "source": [
        "## Pengujian dan Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Sg3Sh3GAPt"
      },
      "outputs": [],
      "source": [
        "# Menurut plot di atas, MAE hasil validasi berhenti menurun secara signifikan setelah 10 epoch.\n",
        "# Peneliti melakukan pengujian sesuai dengan hyperparameter pada kondisi pelatihan.\n",
        "K.clear_session()\n",
        "\n",
        "epoch_test = 100\n",
        "batch_size_test = 5\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data, train_targets,\n",
        "          epochs=epoch_test, batch_size=batch_size_test, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
        "hist = pd.DataFrame(history.history['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnQAEQzoGAQJ"
      },
      "outputs": [],
      "source": [
        "# Melihat skor MAE hasil pengujian.\n",
        "test_mae_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfLEbC49VgOu"
      },
      "outputs": [],
      "source": [
        "# Melihat grafik skor MAE data pengujian versus data validasi.\n",
        "def plot_history():\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.plot(hist, label='Pengujian')\n",
        "    plt.plot(range(1, len(average_mae_history) + 1), average_mae_history, label='Validasi')\n",
        "    plt.legend()\n",
        "    plt.ylim([0.5,6])\n",
        "    plt.xlim()\n",
        "\n",
        "plot_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbaQNr1V_jWr"
      },
      "outputs": [],
      "source": [
        "# Melihat plot nilai riil versus nilai prediksi.\n",
        "test_prediction = model.predict(test_data).flatten()\n",
        "def plot_a():\n",
        "    plt.ylabel('Nilai Prediksi')\n",
        "    plt.xlabel('Nilai Riil')\n",
        "    plt.scatter(test_targets, test_prediction, color='green')\n",
        "    plt.plot([0,50], [0,50], color='red')\n",
        "\n",
        "plot_a()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "33221041-Model-Regresi-FCNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}